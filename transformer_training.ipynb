{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqiLLBFP_Rw5"
      },
      "outputs": [],
      "source": [
        "# Train-Val split for Model Training & Decision on the Model Infrastructure\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_transf, validation = train_test_split(train, stratify=train[[\"outcome\"]], test_size=0.3)\n",
        "\n",
        "#Here we applied the Over/Undersampling. However, as we used none of those in the final approach, we just copy the data here.\n",
        "train_resampled_body, train_resampled_outcome = train_transf[[\"body_transform\"]], train_transf[[\"outcome\"]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Rename columns\n",
        "train_resampled_body[\"text\"] = train_resampled_body[\"body_transform\"]\n",
        "train_resampled_body[\"label\"] = train_resampled_outcome[\"outcome\"]\n",
        "\n",
        "# Create a new DataFrame with selected columns\n",
        "train_transformer = train_resampled_body[[\"label\", \"text\"]]\n",
        "\n",
        "train_data = Dataset.from_pandas(train_transformer)\n",
        "\n",
        "validation[\"text\"] = validation[\"body_transform\"]\n",
        "validation[\"label\"] = validation[\"outcome\"]\n",
        "\n",
        "validation_transformer = validation[[\"label\", \"text\"]]\n",
        "\n",
        "validation_data = Dataset.from_pandas(validation_transformer)\n",
        "\n",
        "\n",
        "# Load the Longformer Model for tokenization.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"LennartKeller/longformer-gottbert-base-8192-aw512\")\n",
        "\n",
        "#Those tokens were added to represent the data structure.\n",
        "special_tokens_dict = {'additional_special_tokens': ['[CNSLR]', '[USER]']}\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "# Function for tokenization, max length is coming from the maximum input length the longformer model can use.\n",
        "def tokenize(dataset):\n",
        "    return tokenizer(dataset[\"text\"], truncation=True, padding=\"max_length\", max_length=8192)\n",
        "\n",
        "# Tokenizing the text\n",
        "train_data_tokenized = train_data.map(tokenize)\n",
        "validation_data_tokenized = validation_data.map(tokenize)\n"
      ],
      "metadata": {
        "id": "vMrVO-5H_aFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC AUC Score as evaluation metric\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import tensorflow as tf\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = tf.math.softmax(pred.predictions, axis=-1)\n",
        "    roc = roc_auc_score(labels, preds[:, 1])\n",
        "    return {'roc' :roc}"
      ],
      "metadata": {
        "id": "c_6Betgg_nVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Class Weighting as selected as technique to handle class imbalance.\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        # forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # compute custom loss\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([9.0, 1.0], device=model.device))\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "4k3pnCo7_pHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model training and evaluation on val set.\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"LennartKeller/longformer-gottbert-base-8192-aw512\", num_labels=2)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    report_to=[],\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size = 2,\n",
        "    per_device_eval_batch_size = 2,\n",
        "    evaluation_strategy = \"epoch\",\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data_tokenized,\n",
        "    eval_dataset=validation_data_tokenized,\n",
        "    compute_metrics=compute_metrics\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "4eclbO1p_ubD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}